{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "f328bf5309404c8a98914d143518c1e8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_3c862931912b4f6bbb7efb5aa735ce0b",
              "IPY_MODEL_9e3bb2b316ca4f64bd54c0680b8f805c",
              "IPY_MODEL_48a559c469474be2b805450576e87d39"
            ],
            "layout": "IPY_MODEL_b73142d877ee470fb0943c162b9429fb"
          }
        },
        "3c862931912b4f6bbb7efb5aa735ce0b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_60b687c318284141a02a14df9e8464e3",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_23873cb4c2d0486aa2f5298ec653e703",
            "value": "Loading‚Äádataset‚Äáfrom‚Äádisk:‚Äá100%"
          }
        },
        "9e3bb2b316ca4f64bd54c0680b8f805c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_338a6f47e7f54328b876eb34221c90a6",
            "max": 33,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_dd82e78ce34a4b719390b3b6356d19eb",
            "value": 33
          }
        },
        "48a559c469474be2b805450576e87d39": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3e90b437dc2f4cfeab0e806b22230dbd",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_5943d50a007e43a6bbed88121acf759c",
            "value": "‚Äá33/33‚Äá[00:00&lt;00:00,‚Äá84.25it/s]"
          }
        },
        "b73142d877ee470fb0943c162b9429fb": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "60b687c318284141a02a14df9e8464e3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "23873cb4c2d0486aa2f5298ec653e703": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "338a6f47e7f54328b876eb34221c90a6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "dd82e78ce34a4b719390b3b6356d19eb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "3e90b437dc2f4cfeab0e806b22230dbd": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5943d50a007e43a6bbed88121acf759c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "f328bf5309404c8a98914d143518c1e8",
            "3c862931912b4f6bbb7efb5aa735ce0b",
            "9e3bb2b316ca4f64bd54c0680b8f805c",
            "48a559c469474be2b805450576e87d39",
            "b73142d877ee470fb0943c162b9429fb",
            "60b687c318284141a02a14df9e8464e3",
            "23873cb4c2d0486aa2f5298ec653e703",
            "338a6f47e7f54328b876eb34221c90a6",
            "dd82e78ce34a4b719390b3b6356d19eb",
            "3e90b437dc2f4cfeab0e806b22230dbd",
            "5943d50a007e43a6bbed88121acf759c"
          ]
        },
        "id": "ArSzCM48Kxuh",
        "outputId": "4bff58fb-774e-4780-ef9d-42c8e1ffc14c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[33mWARNING: Ignoring invalid distribution ~ransformers (/usr/local/lib/python3.12/dist-packages)\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution ~ransformers (/usr/local/lib/python3.12/dist-packages)\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution ~ransformers (/usr/local/lib/python3.12/dist-packages)\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution ~ransformers (/usr/local/lib/python3.12/dist-packages)\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution ~ransformers (/usr/local/lib/python3.12/dist-packages)\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution ~ransformers (/usr/local/lib/python3.12/dist-packages)\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution ~ransformers (/usr/local/lib/python3.12/dist-packages)\u001b[0m\u001b[33m\n",
            "\u001b[0m  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[33mWARNING: Ignoring invalid distribution ~ransformers (/usr/local/lib/python3.12/dist-packages)\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution ~ransformers (/usr/local/lib/python3.12/dist-packages)\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution ~ransformers (/usr/local/lib/python3.12/dist-packages)\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution ~ransformers (/usr/local/lib/python3.12/dist-packages)\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution ~ransformers (/usr/local/lib/python3.12/dist-packages)\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution ~ransformers (/usr/local/lib/python3.12/dist-packages)\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution ~ransformers (/usr/local/lib/python3.12/dist-packages)\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution ~ransformers (/usr/local/lib/python3.12/dist-packages)\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "unsloth 2025.10.1 requires transformers!=4.52.0,!=4.52.1,!=4.52.2,!=4.52.3,!=4.53.0,!=4.54.0,!=4.55.0,!=4.55.1,<=4.56.2,>=4.51.3, but you have transformers 4.57.0 which is incompatible.\n",
            "unsloth-zoo 2025.10.1 requires transformers!=4.52.0,!=4.52.1,!=4.52.2,!=4.52.3,!=4.53.0,!=4.54.0,!=4.55.0,!=4.55.1,<=4.56.2,>=4.51.3, but you have transformers 4.57.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m‚úÖ Pacotes importados com sucesso!\n",
            "\n",
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "üìÅ Diret√≥rios configurados:\n",
            "   Base: /content/drive/MyDrive/tc_fiap_ft1\n",
            "   Output: /content/drive/MyDrive/tc_fiap_ft1/output\n",
            "\n",
            "üñ•Ô∏è  GPU: NVIDIA A100-SXM4-80GB\n",
            "üíæ VRAM: 85 GB\n",
            "\n",
            "üì¶ Carregando modelo LLaMA 3.2-1B...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.\n",
            "/usr/local/lib/python3.12/dist-packages/transformers/quantizers/auto.py:239: UserWarning: You passed `quantization_config` or equivalent parameters to `from_pretrained` but the model you're loading already has a `quantization_config` attribute. The `quantization_config` from the model will be used.\n",
            "  warnings.warn(warning_msg)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Modelo carregado!\n",
            "\n",
            "üîß Aplicando LoRA...\n",
            "‚úÖ LoRA aplicado!\n",
            "trainable params: 3,407,872 || all params: 1,239,222,272 || trainable%: 0.2750\n",
            "\n",
            "‚ôªÔ∏è  Carregando datasets tokenizados do cache...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading dataset from disk:   0%|          | 0/33 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "f328bf5309404c8a98914d143518c1e8"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-2590405546.py:229: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
            "  trainer = Trainer(\n",
            "The model is already on multiple devices. Skipping the move to device specified in `args`.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Train: 1,229,886 | Eval: 136,655\n",
            "\n",
            "‚öôÔ∏è  Configurando treinamento...\n",
            "\n",
            "‚úÖ Trainer configurado!\n",
            "üéØ Total steps: 2000\n",
            "‚è±Ô∏è  Tempo estimado: ~33 minutos\n",
            "\n",
            "====================================================================================================\n",
            "                                      üî• INICIANDO TREINAMENTO                                       \n",
            "====================================================================================================\n",
            "\n",
            "‚öôÔ∏è  Configura√ß√£o:\n",
            "   ‚Ä¢ Batch Size: 24\n",
            "   ‚Ä¢ Max Length: 768 tokens\n",
            "   ‚Ä¢ Learning Rate: 0.0005\n",
            "   ‚Ä¢ Steps: 2000\n",
            "   ‚Ä¢ Eval: A cada 500 steps\n",
            "   ‚Ä¢ Checkpoints: Desabilitados\n",
            "   ‚Ä¢ Precis√£o: BFloat16\n",
            "\n",
            "‚ö° Velocidade esperada: ~60 steps/min\n",
            "‚è±Ô∏è  ETA: ~33 minutos\n",
            "====================================================================================================\n",
            "\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='501' max='2000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [ 501/2000 15:50 < 47:36, 0.52 it/s, Epoch 0.01/1]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "  </tbody>\n",
              "</table><p>\n",
              "    <div>\n",
              "      \n",
              "      <progress value='3505' max='5694' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [3505/5694 34:45 < 21:42, 1.68 it/s]\n",
              "    </div>\n",
              "    "
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "# ====================================================================\n",
        "# FINE-TUNING LLAMA 3.2-1B - A100 VELOCIDADE M√ÅXIMA\n",
        "# ====================================================================\n",
        "# ‚úÖ S√ì RODAR - SEM CHECKPOINTS - OTIMIZADO PARA VELOCIDADE\n",
        "# ====================================================================\n",
        "\n",
        "# 1Ô∏è‚É£ INSTALA√á√ÉO R√ÅPIDA\n",
        "!pip install -q --upgrade pip\n",
        "!pip install -q accelerate bitsandbytes einops sentencepiece\n",
        "!pip install -q git+https://github.com/huggingface/peft.git\n",
        "!pip install -q unsloth xformers trl\n",
        "!pip install -q --upgrade transformers datasets tqdm\n",
        "\n",
        "# 2Ô∏è‚É£ IMPORTS\n",
        "import os\n",
        "import time\n",
        "import torch\n",
        "import gc\n",
        "from pathlib import Path\n",
        "from datasets import load_dataset, load_from_disk\n",
        "from transformers import (\n",
        "    AutoTokenizer,\n",
        "    AutoModelForCausalLM,\n",
        "    TrainingArguments,\n",
        "    Trainer,\n",
        "    DataCollatorForLanguageModeling,\n",
        ")\n",
        "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\n",
        "\n",
        "print(\"‚úÖ Pacotes importados com sucesso!\\n\")\n",
        "\n",
        "# 3Ô∏è‚É£ MONTAR DRIVE\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# 4Ô∏è‚É£ CONFIGURAR DIRET√ìRIOS\n",
        "BASE_DIR = Path(\"/content/drive/MyDrive/tc_fiap_ft1\")\n",
        "BASE_DIR.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "INPUT_FILE = BASE_DIR / \"trn.json\"\n",
        "CHUNKS_DIR = BASE_DIR / \"chunks\"\n",
        "OUTPUT_DIR = BASE_DIR / \"output\"\n",
        "TOKENIZED_DIR = BASE_DIR / \"tokenized_datasets\"\n",
        "\n",
        "for d in [CHUNKS_DIR, OUTPUT_DIR, TOKENIZED_DIR]:\n",
        "    d.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "print(f\"üìÅ Diret√≥rios configurados:\")\n",
        "print(f\"   Base: {BASE_DIR}\")\n",
        "print(f\"   Output: {OUTPUT_DIR}\\n\")\n",
        "\n",
        "# 5Ô∏è‚É£ VERIFICAR GPU\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "if torch.cuda.is_available():\n",
        "    gpu_name = torch.cuda.get_device_name(0)\n",
        "    gpu_memory = torch.cuda.get_device_properties(0).total_memory / 1e9\n",
        "    print(f\"üñ•Ô∏è  GPU: {gpu_name}\")\n",
        "    print(f\"üíæ VRAM: {gpu_memory:.0f} GB\\n\")\n",
        "else:\n",
        "    print(\"‚ö†Ô∏è  Sem GPU detectada!\\n\")\n",
        "\n",
        "# 6Ô∏è‚É£ CARREGAR MODELO E TOKENIZER\n",
        "print(\"üì¶ Carregando modelo LLaMA 3.2-1B...\")\n",
        "model_name = \"unsloth/Llama-3.2-1B-bnb-4bit\"\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "if tokenizer.pad_token is None:\n",
        "    tokenizer.pad_token = tokenizer.eos_token\n",
        "tokenizer.padding_side = \"right\"\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_name,\n",
        "    load_in_4bit=True,\n",
        "    device_map=\"auto\",\n",
        "    torch_dtype=torch.bfloat16,\n",
        ")\n",
        "\n",
        "model = prepare_model_for_kbit_training(model)\n",
        "model.gradient_checkpointing_enable()\n",
        "\n",
        "print(\"‚úÖ Modelo carregado!\\n\")\n",
        "\n",
        "# 7Ô∏è‚É£ CONFIGURAR LORA (Otimizado para velocidade)\n",
        "print(\"üîß Aplicando LoRA...\")\n",
        "lora_config = LoraConfig(\n",
        "    r=16,  # Rank reduzido para velocidade\n",
        "    lora_alpha=32,\n",
        "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\"],  # Menos m√≥dulos = mais r√°pido\n",
        "    lora_dropout=0.05,\n",
        "    bias=\"none\",\n",
        "    task_type=\"CAUSAL_LM\"\n",
        ")\n",
        "\n",
        "model = get_peft_model(model, lora_config)\n",
        "print(f\"‚úÖ LoRA aplicado!\")\n",
        "model.print_trainable_parameters()\n",
        "print()\n",
        "\n",
        "# 8Ô∏è‚É£ CARREGAR OU PROCESSAR DATASETS\n",
        "train_path = TOKENIZED_DIR / \"train\"\n",
        "eval_path = TOKENIZED_DIR / \"eval\"\n",
        "\n",
        "if train_path.exists() and eval_path.exists():\n",
        "    print(\"‚ôªÔ∏è  Carregando datasets tokenizados do cache...\")\n",
        "    tokenized_train = load_from_disk(str(train_path))\n",
        "    tokenized_eval = load_from_disk(str(eval_path))\n",
        "    print(f\"‚úÖ Train: {len(tokenized_train):,} | Eval: {len(tokenized_eval):,}\\n\")\n",
        "\n",
        "else:\n",
        "    print(\"üîÑ Processando datasets pela primeira vez...\\n\")\n",
        "\n",
        "    # Carregar JSONs\n",
        "    json_files = sorted([str(f) for f in CHUNKS_DIR.glob(\"*.json\")])\n",
        "    if not json_files:\n",
        "        print(f\"‚ùå Nenhum arquivo JSON em {CHUNKS_DIR}\")\n",
        "        print(f\"üí° Coloque seus arquivos chunk_*.json em: {CHUNKS_DIR}\\n\")\n",
        "        raise FileNotFoundError(\"Arquivos JSON n√£o encontrados!\")\n",
        "\n",
        "    print(f\"üìä Carregando {len(json_files)} arquivo(s)...\")\n",
        "    dataset = load_dataset(\"json\", data_files=json_files, split=\"train\")\n",
        "\n",
        "    # Split train/eval\n",
        "    split = dataset.train_test_split(test_size=0.05, seed=42)  # 5% eval (menos = mais r√°pido)\n",
        "    train_dataset = split[\"train\"]\n",
        "    eval_dataset = split[\"test\"]\n",
        "\n",
        "    print(f\"üìä Train: {len(train_dataset):,} | Eval: {len(eval_dataset):,}\")\n",
        "\n",
        "    # Tokeniza√ß√£o R√ÅPIDA\n",
        "    max_length = 768  # 512 tokens = 2x mais r√°pido que 1024\n",
        "\n",
        "    def preprocess(examples):\n",
        "        titles = examples.get(\"title\", [\"\"] * len(examples.get(\"content\", [])))\n",
        "        contents = examples.get(\"content\", [])\n",
        "        texts = [f\"### T√≠tulo: {t}\\n### Conte√∫do: {c}\" for t, c in zip(titles, contents)]\n",
        "\n",
        "        tokenized = tokenizer(\n",
        "            texts,\n",
        "            truncation=True,\n",
        "            padding=\"max_length\",\n",
        "            max_length=max_length,\n",
        "            return_tensors=None\n",
        "        )\n",
        "        tokenized[\"labels\"] = tokenized[\"input_ids\"].copy()\n",
        "        return tokenized\n",
        "\n",
        "    print(\"üîÑ Tokenizando (isso pode levar 10-15 min)...\")\n",
        "    tokenized_train = train_dataset.map(\n",
        "        preprocess,\n",
        "        batched=True,\n",
        "        batch_size=2000,\n",
        "        num_proc=4,\n",
        "        remove_columns=train_dataset.column_names,\n",
        "        desc=\"Tokenizando treino\"\n",
        "    )\n",
        "\n",
        "    tokenized_eval = eval_dataset.map(\n",
        "        preprocess,\n",
        "        batched=True,\n",
        "        batch_size=2000,\n",
        "        num_proc=4,\n",
        "        remove_columns=eval_dataset.column_names,\n",
        "        desc=\"Tokenizando eval\"\n",
        "    )\n",
        "\n",
        "    # Salvar para pr√≥ximas execu√ß√µes\n",
        "    print(\"üíæ Salvando cache...\")\n",
        "    tokenized_train.save_to_disk(str(train_path))\n",
        "    tokenized_eval.save_to_disk(str(eval_path))\n",
        "    print(f\"‚úÖ Cache salvo em: {TOKENIZED_DIR}\\n\")\n",
        "\n",
        "# 9Ô∏è‚É£ DATA COLLATOR\n",
        "data_collator = DataCollatorForLanguageModeling(\n",
        "    tokenizer=tokenizer,\n",
        "    mlm=False,\n",
        "    return_tensors=\"pt\"\n",
        ")\n",
        "\n",
        "# üîü TRAINING ARGUMENTS - M√ÅXIMA VELOCIDADE\n",
        "print(\"‚öôÔ∏è  Configurando treinamento...\\n\")\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"/tmp/training_output\",  # /tmp = mais r√°pido que Drive\n",
        "\n",
        "    # BATCH - Otimizado para A100 sem estourar RAM\n",
        "    per_device_train_batch_size=24,  # M√°ximo seguro para A100\n",
        "    per_device_eval_batch_size=24,\n",
        "    gradient_accumulation_steps=1,  # Sem acumula√ß√£o = mais r√°pido\n",
        "\n",
        "    # LEARNING\n",
        "    learning_rate=5e-4,\n",
        "    lr_scheduler_type=\"cosine\",\n",
        "    warmup_ratio=0.03,\n",
        "\n",
        "    # PRECIS√ÉO\n",
        "    bf16=True,\n",
        "    bf16_full_eval=True,\n",
        "\n",
        "    # LOGGING M√çNIMO\n",
        "    logging_steps=100,\n",
        "    logging_first_step=True,\n",
        "\n",
        "    # SEM SALVAMENTO DURANTE TREINO\n",
        "    save_strategy=\"no\",\n",
        "    save_steps=999999,\n",
        "\n",
        "    # AVALIA√á√ÉO M√çNIMA\n",
        "    eval_strategy=\"steps\",\n",
        "    eval_steps=500,  # Apenas 4 avalia√ß√µes\n",
        "\n",
        "    # STEPS\n",
        "    num_train_epochs=1,\n",
        "    max_steps=2000,\n",
        "\n",
        "    # OTIMIZA√á√ïES\n",
        "    optim=\"adamw_torch_fused\",\n",
        "    gradient_checkpointing=True,\n",
        "\n",
        "    # DATALOADER R√ÅPIDO\n",
        "    dataloader_num_workers=2,  # 2 workers = balan√ßo velocidade/RAM\n",
        "    dataloader_pin_memory=True,\n",
        "\n",
        "    # SEM EXTRAS\n",
        "    report_to=\"none\",\n",
        "    load_best_model_at_end=False,\n",
        "    disable_tqdm=False,\n",
        ")\n",
        "\n",
        "# 1Ô∏è‚É£1Ô∏è‚É£ CRIAR TRAINER\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    train_dataset=tokenized_train,\n",
        "    eval_dataset=tokenized_eval,\n",
        "    tokenizer=tokenizer,\n",
        "    args=training_args,\n",
        "    data_collator=data_collator\n",
        ")\n",
        "\n",
        "print(\"‚úÖ Trainer configurado!\")\n",
        "print(f\"üéØ Total steps: {training_args.max_steps}\")\n",
        "print(f\"‚è±Ô∏è  Tempo estimado: ~{training_args.max_steps/60:.0f} minutos\\n\")\n",
        "\n",
        "# 1Ô∏è‚É£2Ô∏è‚É£ TREINAR\n",
        "print(\"=\"*100)\n",
        "print(\"üî• INICIANDO TREINAMENTO\".center(100))\n",
        "print(\"=\"*100)\n",
        "print(f\"\\n‚öôÔ∏è  Configura√ß√£o:\")\n",
        "print(f\"   ‚Ä¢ Batch Size: {training_args.per_device_train_batch_size}\")\n",
        "print(f\"   ‚Ä¢ Max Length: 768 tokens\")\n",
        "print(f\"   ‚Ä¢ Learning Rate: {training_args.learning_rate}\")\n",
        "print(f\"   ‚Ä¢ Steps: {training_args.max_steps}\")\n",
        "print(f\"   ‚Ä¢ Eval: A cada 500 steps\")\n",
        "print(f\"   ‚Ä¢ Checkpoints: Desabilitados\")\n",
        "print(f\"   ‚Ä¢ Precis√£o: BFloat16\")\n",
        "print(f\"\\n‚ö° Velocidade esperada: ~60 steps/min\")\n",
        "print(f\"‚è±Ô∏è  ETA: ~{training_args.max_steps/60:.0f} minutos\")\n",
        "print(\"=\"*100 + \"\\n\")\n",
        "\n",
        "start_time = time.time()\n",
        "\n",
        "try:\n",
        "    trainer.train()\n",
        "    print(\"\\n‚úÖ Treinamento conclu√≠do!\\n\")\n",
        "\n",
        "except KeyboardInterrupt:\n",
        "    print(\"\\n‚ö†Ô∏è  Treinamento interrompido pelo usu√°rio!\\n\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"\\n‚ùå Erro: {e}\\n\")\n",
        "    raise\n",
        "\n",
        "finally:\n",
        "    training_time = (time.time() - start_time) / 60\n",
        "    print(f\"‚è±Ô∏è  Tempo de treinamento: {training_time:.1f} minutos\")\n",
        "\n",
        "    # Limpar mem√≥ria\n",
        "    gc.collect()\n",
        "    torch.cuda.empty_cache()\n",
        "\n",
        "# 1Ô∏è‚É£3Ô∏è‚É£ SALVAR MODELO FINAL\n",
        "print(\"\\n\" + \"=\"*100)\n",
        "print(\"üíæ SALVANDO MODELO FINAL\".center(100))\n",
        "print(\"=\"*100 + \"\\n\")\n",
        "\n",
        "final_model_path = OUTPUT_DIR / \"final_model\"\n",
        "\n",
        "try:\n",
        "    model.save_pretrained(str(final_model_path))\n",
        "    tokenizer.save_pretrained(str(final_model_path))\n",
        "\n",
        "    print(f\"‚úÖ Modelo salvo em: {final_model_path}\\n\")\n",
        "\n",
        "    # Mostrar arquivos\n",
        "    print(\"üìÅ Arquivos gerados:\")\n",
        "    for file in sorted(final_model_path.iterdir()):\n",
        "        size = file.stat().st_size / (1024*1024)\n",
        "        print(f\"   ‚Ä¢ {file.name}: {size:.1f} MB\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"‚ùå Erro ao salvar: {e}\\n\")\n",
        "    raise\n",
        "\n",
        "# 1Ô∏è‚É£4Ô∏è‚É£ ESTAT√çSTICAS FINAIS\n",
        "print(\"\\n\" + \"=\"*100)\n",
        "print(\"üìä RESUMO DO TREINAMENTO\".center(100))\n",
        "print(\"=\"*100 + \"\\n\")\n",
        "\n",
        "if hasattr(trainer.state, 'log_history') and trainer.state.log_history:\n",
        "    logs = trainer.state.log_history\n",
        "    train_losses = [log['loss'] for log in logs if 'loss' in log]\n",
        "    eval_losses = [log['eval_loss'] for log in logs if 'eval_loss' in log]\n",
        "\n",
        "    if train_losses:\n",
        "        print(\"‚ïî\" + \"‚ïê\"*98 + \"‚ïó\")\n",
        "        print(\"‚ïë\" + \" üìà M√âTRICAS FINAIS \".center(98) + \"‚ïë\")\n",
        "        print(\"‚ï†\" + \"‚ïê\"*98 + \"‚ï£\")\n",
        "        print(f\"‚ïë  üéØ Steps: {trainer.state.global_step:,}/{training_args.max_steps:,}\".ljust(99) + \"‚ïë\")\n",
        "        print(f\"‚ïë  ‚è±Ô∏è  Tempo: {training_time:.1f} minutos\".ljust(99) + \"‚ïë\")\n",
        "        print(f\"‚ïë  ‚ö° Velocidade: {trainer.state.global_step/training_time:.1f} steps/min\".ljust(99) + \"‚ïë\")\n",
        "        print(\"‚ïë\" + \" \"*98 + \"‚ïë\")\n",
        "        print(f\"‚ïë  üìâ Loss Inicial: {train_losses[0]:.4f}\".ljust(99) + \"‚ïë\")\n",
        "        print(f\"‚ïë  üìâ Loss Final: {train_losses[-1]:.4f}\".ljust(99) + \"‚ïë\")\n",
        "\n",
        "        if eval_losses:\n",
        "            print(f\"‚ïë  üìä Melhor Eval Loss: {min(eval_losses):.4f}\".ljust(99) + \"‚ïë\")\n",
        "\n",
        "        improvement = ((train_losses[0] - train_losses[-1]) / train_losses[0]) * 100\n",
        "        print(f\"‚ïë  üìà Melhoria: {improvement:.1f}%\".ljust(99) + \"‚ïë\")\n",
        "        print(\"‚ïö\" + \"‚ïê\"*98 + \"‚ïù\")\n",
        "\n",
        "print(\"\\nüéâ TREINAMENTO CONCLU√çDO COM SUCESSO!\")\n",
        "print(f\"üíæ Modelo pronto em: {final_model_path}\")\n",
        "print(f\"üìä Datasets tokenizados em: {TOKENIZED_DIR}\")\n",
        "print(\"\\n‚úÖ Pr√≥xima execu√ß√£o ser√° mais r√°pida (usa cache)!\")\n",
        "print(\"=\"*100 + \"\\n\")"
      ]
    }
  ]
}