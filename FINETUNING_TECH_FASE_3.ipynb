{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "f328bf5309404c8a98914d143518c1e8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_3c862931912b4f6bbb7efb5aa735ce0b",
              "IPY_MODEL_9e3bb2b316ca4f64bd54c0680b8f805c",
              "IPY_MODEL_48a559c469474be2b805450576e87d39"
            ],
            "layout": "IPY_MODEL_b73142d877ee470fb0943c162b9429fb"
          }
        },
        "3c862931912b4f6bbb7efb5aa735ce0b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_60b687c318284141a02a14df9e8464e3",
            "placeholder": "​",
            "style": "IPY_MODEL_23873cb4c2d0486aa2f5298ec653e703",
            "value": "Loading dataset from disk: 100%"
          }
        },
        "9e3bb2b316ca4f64bd54c0680b8f805c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_338a6f47e7f54328b876eb34221c90a6",
            "max": 33,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_dd82e78ce34a4b719390b3b6356d19eb",
            "value": 33
          }
        },
        "48a559c469474be2b805450576e87d39": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3e90b437dc2f4cfeab0e806b22230dbd",
            "placeholder": "​",
            "style": "IPY_MODEL_5943d50a007e43a6bbed88121acf759c",
            "value": " 33/33 [00:00&lt;00:00, 84.25it/s]"
          }
        },
        "b73142d877ee470fb0943c162b9429fb": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "60b687c318284141a02a14df9e8464e3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "23873cb4c2d0486aa2f5298ec653e703": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "338a6f47e7f54328b876eb34221c90a6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "dd82e78ce34a4b719390b3b6356d19eb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "3e90b437dc2f4cfeab0e806b22230dbd": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5943d50a007e43a6bbed88121acf759c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "f328bf5309404c8a98914d143518c1e8",
            "3c862931912b4f6bbb7efb5aa735ce0b",
            "9e3bb2b316ca4f64bd54c0680b8f805c",
            "48a559c469474be2b805450576e87d39",
            "b73142d877ee470fb0943c162b9429fb",
            "60b687c318284141a02a14df9e8464e3",
            "23873cb4c2d0486aa2f5298ec653e703",
            "338a6f47e7f54328b876eb34221c90a6",
            "dd82e78ce34a4b719390b3b6356d19eb",
            "3e90b437dc2f4cfeab0e806b22230dbd",
            "5943d50a007e43a6bbed88121acf759c"
          ]
        },
        "id": "ArSzCM48Kxuh",
        "outputId": "4bff58fb-774e-4780-ef9d-42c8e1ffc14c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[33mWARNING: Ignoring invalid distribution ~ransformers (/usr/local/lib/python3.12/dist-packages)\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution ~ransformers (/usr/local/lib/python3.12/dist-packages)\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution ~ransformers (/usr/local/lib/python3.12/dist-packages)\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution ~ransformers (/usr/local/lib/python3.12/dist-packages)\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution ~ransformers (/usr/local/lib/python3.12/dist-packages)\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution ~ransformers (/usr/local/lib/python3.12/dist-packages)\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution ~ransformers (/usr/local/lib/python3.12/dist-packages)\u001b[0m\u001b[33m\n",
            "\u001b[0m  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[33mWARNING: Ignoring invalid distribution ~ransformers (/usr/local/lib/python3.12/dist-packages)\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution ~ransformers (/usr/local/lib/python3.12/dist-packages)\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution ~ransformers (/usr/local/lib/python3.12/dist-packages)\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution ~ransformers (/usr/local/lib/python3.12/dist-packages)\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution ~ransformers (/usr/local/lib/python3.12/dist-packages)\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution ~ransformers (/usr/local/lib/python3.12/dist-packages)\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution ~ransformers (/usr/local/lib/python3.12/dist-packages)\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution ~ransformers (/usr/local/lib/python3.12/dist-packages)\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "unsloth 2025.10.1 requires transformers!=4.52.0,!=4.52.1,!=4.52.2,!=4.52.3,!=4.53.0,!=4.54.0,!=4.55.0,!=4.55.1,<=4.56.2,>=4.51.3, but you have transformers 4.57.0 which is incompatible.\n",
            "unsloth-zoo 2025.10.1 requires transformers!=4.52.0,!=4.52.1,!=4.52.2,!=4.52.3,!=4.53.0,!=4.54.0,!=4.55.0,!=4.55.1,<=4.56.2,>=4.51.3, but you have transformers 4.57.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m✅ Pacotes importados com sucesso!\n",
            "\n",
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "📁 Diretórios configurados:\n",
            "   Base: /content/drive/MyDrive/tc_fiap_ft1\n",
            "   Output: /content/drive/MyDrive/tc_fiap_ft1/output\n",
            "\n",
            "🖥️  GPU: NVIDIA A100-SXM4-80GB\n",
            "💾 VRAM: 85 GB\n",
            "\n",
            "📦 Carregando modelo LLaMA 3.2-1B...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.\n",
            "/usr/local/lib/python3.12/dist-packages/transformers/quantizers/auto.py:239: UserWarning: You passed `quantization_config` or equivalent parameters to `from_pretrained` but the model you're loading already has a `quantization_config` attribute. The `quantization_config` from the model will be used.\n",
            "  warnings.warn(warning_msg)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Modelo carregado!\n",
            "\n",
            "🔧 Aplicando LoRA...\n",
            "✅ LoRA aplicado!\n",
            "trainable params: 3,407,872 || all params: 1,239,222,272 || trainable%: 0.2750\n",
            "\n",
            "♻️  Carregando datasets tokenizados do cache...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading dataset from disk:   0%|          | 0/33 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "f328bf5309404c8a98914d143518c1e8"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-2590405546.py:229: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
            "  trainer = Trainer(\n",
            "The model is already on multiple devices. Skipping the move to device specified in `args`.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Train: 1,229,886 | Eval: 136,655\n",
            "\n",
            "⚙️  Configurando treinamento...\n",
            "\n",
            "✅ Trainer configurado!\n",
            "🎯 Total steps: 2000\n",
            "⏱️  Tempo estimado: ~33 minutos\n",
            "\n",
            "====================================================================================================\n",
            "                                      🔥 INICIANDO TREINAMENTO                                       \n",
            "====================================================================================================\n",
            "\n",
            "⚙️  Configuração:\n",
            "   • Batch Size: 24\n",
            "   • Max Length: 768 tokens\n",
            "   • Learning Rate: 0.0005\n",
            "   • Steps: 2000\n",
            "   • Eval: A cada 500 steps\n",
            "   • Checkpoints: Desabilitados\n",
            "   • Precisão: BFloat16\n",
            "\n",
            "⚡ Velocidade esperada: ~60 steps/min\n",
            "⏱️  ETA: ~33 minutos\n",
            "====================================================================================================\n",
            "\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='501' max='2000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [ 501/2000 15:50 < 47:36, 0.52 it/s, Epoch 0.01/1]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "  </tbody>\n",
              "</table><p>\n",
              "    <div>\n",
              "      \n",
              "      <progress value='3505' max='5694' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [3505/5694 34:45 < 21:42, 1.68 it/s]\n",
              "    </div>\n",
              "    "
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "# ====================================================================\n",
        "# FINE-TUNING LLAMA 3.2-1B - A100 VELOCIDADE MÁXIMA\n",
        "# ====================================================================\n",
        "# ✅ SÓ RODAR - SEM CHECKPOINTS - OTIMIZADO PARA VELOCIDADE\n",
        "# ====================================================================\n",
        "\n",
        "# 1️⃣ INSTALAÇÃO RÁPIDA\n",
        "!pip install -q --upgrade pip\n",
        "!pip install -q accelerate bitsandbytes einops sentencepiece\n",
        "!pip install -q git+https://github.com/huggingface/peft.git\n",
        "!pip install -q unsloth xformers trl\n",
        "!pip install -q --upgrade transformers datasets tqdm\n",
        "\n",
        "# 2️⃣ IMPORTS\n",
        "import os\n",
        "import time\n",
        "import torch\n",
        "import gc\n",
        "from pathlib import Path\n",
        "from datasets import load_dataset, load_from_disk\n",
        "from transformers import (\n",
        "    AutoTokenizer,\n",
        "    AutoModelForCausalLM,\n",
        "    TrainingArguments,\n",
        "    Trainer,\n",
        "    DataCollatorForLanguageModeling,\n",
        ")\n",
        "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\n",
        "\n",
        "print(\"✅ Pacotes importados com sucesso!\\n\")\n",
        "\n",
        "# 3️⃣ MONTAR DRIVE\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# 4️⃣ CONFIGURAR DIRETÓRIOS\n",
        "BASE_DIR = Path(\"/content/drive/MyDrive/tc_fiap_ft1\")\n",
        "BASE_DIR.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "INPUT_FILE = BASE_DIR / \"trn.json\"\n",
        "CHUNKS_DIR = BASE_DIR / \"chunks\"\n",
        "OUTPUT_DIR = BASE_DIR / \"output\"\n",
        "TOKENIZED_DIR = BASE_DIR / \"tokenized_datasets\"\n",
        "\n",
        "for d in [CHUNKS_DIR, OUTPUT_DIR, TOKENIZED_DIR]:\n",
        "    d.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "print(f\"📁 Diretórios configurados:\")\n",
        "print(f\"   Base: {BASE_DIR}\")\n",
        "print(f\"   Output: {OUTPUT_DIR}\\n\")\n",
        "\n",
        "# 5️⃣ VERIFICAR GPU\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "if torch.cuda.is_available():\n",
        "    gpu_name = torch.cuda.get_device_name(0)\n",
        "    gpu_memory = torch.cuda.get_device_properties(0).total_memory / 1e9\n",
        "    print(f\"🖥️  GPU: {gpu_name}\")\n",
        "    print(f\"💾 VRAM: {gpu_memory:.0f} GB\\n\")\n",
        "else:\n",
        "    print(\"⚠️  Sem GPU detectada!\\n\")\n",
        "\n",
        "# 6️⃣ CARREGAR MODELO E TOKENIZER\n",
        "print(\"📦 Carregando modelo LLaMA 3.2-1B...\")\n",
        "model_name = \"unsloth/Llama-3.2-1B-bnb-4bit\"\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "if tokenizer.pad_token is None:\n",
        "    tokenizer.pad_token = tokenizer.eos_token\n",
        "tokenizer.padding_side = \"right\"\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_name,\n",
        "    load_in_4bit=True,\n",
        "    device_map=\"auto\",\n",
        "    torch_dtype=torch.bfloat16,\n",
        ")\n",
        "\n",
        "model = prepare_model_for_kbit_training(model)\n",
        "model.gradient_checkpointing_enable()\n",
        "\n",
        "print(\"✅ Modelo carregado!\\n\")\n",
        "\n",
        "# 7️⃣ CONFIGURAR LORA (Otimizado para velocidade)\n",
        "print(\"🔧 Aplicando LoRA...\")\n",
        "lora_config = LoraConfig(\n",
        "    r=16,  # Rank reduzido para velocidade\n",
        "    lora_alpha=32,\n",
        "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\"],  # Menos módulos = mais rápido\n",
        "    lora_dropout=0.05,\n",
        "    bias=\"none\",\n",
        "    task_type=\"CAUSAL_LM\"\n",
        ")\n",
        "\n",
        "model = get_peft_model(model, lora_config)\n",
        "print(f\"✅ LoRA aplicado!\")\n",
        "model.print_trainable_parameters()\n",
        "print()\n",
        "\n",
        "# 8️⃣ CARREGAR OU PROCESSAR DATASETS\n",
        "train_path = TOKENIZED_DIR / \"train\"\n",
        "eval_path = TOKENIZED_DIR / \"eval\"\n",
        "\n",
        "if train_path.exists() and eval_path.exists():\n",
        "    print(\"♻️  Carregando datasets tokenizados do cache...\")\n",
        "    tokenized_train = load_from_disk(str(train_path))\n",
        "    tokenized_eval = load_from_disk(str(eval_path))\n",
        "    print(f\"✅ Train: {len(tokenized_train):,} | Eval: {len(tokenized_eval):,}\\n\")\n",
        "\n",
        "else:\n",
        "    print(\"🔄 Processando datasets pela primeira vez...\\n\")\n",
        "\n",
        "    # Carregar JSONs\n",
        "    json_files = sorted([str(f) for f in CHUNKS_DIR.glob(\"*.json\")])\n",
        "    if not json_files:\n",
        "        print(f\"❌ Nenhum arquivo JSON em {CHUNKS_DIR}\")\n",
        "        print(f\"💡 Coloque seus arquivos chunk_*.json em: {CHUNKS_DIR}\\n\")\n",
        "        raise FileNotFoundError(\"Arquivos JSON não encontrados!\")\n",
        "\n",
        "    print(f\"📊 Carregando {len(json_files)} arquivo(s)...\")\n",
        "    dataset = load_dataset(\"json\", data_files=json_files, split=\"train\")\n",
        "\n",
        "    # Split train/eval\n",
        "    split = dataset.train_test_split(test_size=0.05, seed=42)  # 5% eval (menos = mais rápido)\n",
        "    train_dataset = split[\"train\"]\n",
        "    eval_dataset = split[\"test\"]\n",
        "\n",
        "    print(f\"📊 Train: {len(train_dataset):,} | Eval: {len(eval_dataset):,}\")\n",
        "\n",
        "    # Tokenização RÁPIDA\n",
        "    max_length = 768  # 512 tokens = 2x mais rápido que 1024\n",
        "\n",
        "    def preprocess(examples):\n",
        "        titles = examples.get(\"title\", [\"\"] * len(examples.get(\"content\", [])))\n",
        "        contents = examples.get(\"content\", [])\n",
        "        texts = [f\"### Título: {t}\\n### Conteúdo: {c}\" for t, c in zip(titles, contents)]\n",
        "\n",
        "        tokenized = tokenizer(\n",
        "            texts,\n",
        "            truncation=True,\n",
        "            padding=\"max_length\",\n",
        "            max_length=max_length,\n",
        "            return_tensors=None\n",
        "        )\n",
        "        tokenized[\"labels\"] = tokenized[\"input_ids\"].copy()\n",
        "        return tokenized\n",
        "\n",
        "    print(\"🔄 Tokenizando (isso pode levar 10-15 min)...\")\n",
        "    tokenized_train = train_dataset.map(\n",
        "        preprocess,\n",
        "        batched=True,\n",
        "        batch_size=2000,\n",
        "        num_proc=4,\n",
        "        remove_columns=train_dataset.column_names,\n",
        "        desc=\"Tokenizando treino\"\n",
        "    )\n",
        "\n",
        "    tokenized_eval = eval_dataset.map(\n",
        "        preprocess,\n",
        "        batched=True,\n",
        "        batch_size=2000,\n",
        "        num_proc=4,\n",
        "        remove_columns=eval_dataset.column_names,\n",
        "        desc=\"Tokenizando eval\"\n",
        "    )\n",
        "\n",
        "    # Salvar para próximas execuções\n",
        "    print(\"💾 Salvando cache...\")\n",
        "    tokenized_train.save_to_disk(str(train_path))\n",
        "    tokenized_eval.save_to_disk(str(eval_path))\n",
        "    print(f\"✅ Cache salvo em: {TOKENIZED_DIR}\\n\")\n",
        "\n",
        "# 9️⃣ DATA COLLATOR\n",
        "data_collator = DataCollatorForLanguageModeling(\n",
        "    tokenizer=tokenizer,\n",
        "    mlm=False,\n",
        "    return_tensors=\"pt\"\n",
        ")\n",
        "\n",
        "# 🔟 TRAINING ARGUMENTS - MÁXIMA VELOCIDADE\n",
        "print(\"⚙️  Configurando treinamento...\\n\")\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"/tmp/training_output\",  # /tmp = mais rápido que Drive\n",
        "\n",
        "    # BATCH - Otimizado para A100 sem estourar RAM\n",
        "    per_device_train_batch_size=24,  # Máximo seguro para A100\n",
        "    per_device_eval_batch_size=24,\n",
        "    gradient_accumulation_steps=1,  # Sem acumulação = mais rápido\n",
        "\n",
        "    # LEARNING\n",
        "    learning_rate=5e-4,\n",
        "    lr_scheduler_type=\"cosine\",\n",
        "    warmup_ratio=0.03,\n",
        "\n",
        "    # PRECISÃO\n",
        "    bf16=True,\n",
        "    bf16_full_eval=True,\n",
        "\n",
        "    # LOGGING MÍNIMO\n",
        "    logging_steps=100,\n",
        "    logging_first_step=True,\n",
        "\n",
        "    # SEM SALVAMENTO DURANTE TREINO\n",
        "    save_strategy=\"no\",\n",
        "    save_steps=999999,\n",
        "\n",
        "    # AVALIAÇÃO MÍNIMA\n",
        "    eval_strategy=\"steps\",\n",
        "    eval_steps=500,  # Apenas 4 avaliações\n",
        "\n",
        "    # STEPS\n",
        "    num_train_epochs=1,\n",
        "    max_steps=2000,\n",
        "\n",
        "    # OTIMIZAÇÕES\n",
        "    optim=\"adamw_torch_fused\",\n",
        "    gradient_checkpointing=True,\n",
        "\n",
        "    # DATALOADER RÁPIDO\n",
        "    dataloader_num_workers=2,  # 2 workers = balanço velocidade/RAM\n",
        "    dataloader_pin_memory=True,\n",
        "\n",
        "    # SEM EXTRAS\n",
        "    report_to=\"none\",\n",
        "    load_best_model_at_end=False,\n",
        "    disable_tqdm=False,\n",
        ")\n",
        "\n",
        "# 1️⃣1️⃣ CRIAR TRAINER\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    train_dataset=tokenized_train,\n",
        "    eval_dataset=tokenized_eval,\n",
        "    tokenizer=tokenizer,\n",
        "    args=training_args,\n",
        "    data_collator=data_collator\n",
        ")\n",
        "\n",
        "print(\"✅ Trainer configurado!\")\n",
        "print(f\"🎯 Total steps: {training_args.max_steps}\")\n",
        "print(f\"⏱️  Tempo estimado: ~{training_args.max_steps/60:.0f} minutos\\n\")\n",
        "\n",
        "# 1️⃣2️⃣ TREINAR\n",
        "print(\"=\"*100)\n",
        "print(\"🔥 INICIANDO TREINAMENTO\".center(100))\n",
        "print(\"=\"*100)\n",
        "print(f\"\\n⚙️  Configuração:\")\n",
        "print(f\"   • Batch Size: {training_args.per_device_train_batch_size}\")\n",
        "print(f\"   • Max Length: 768 tokens\")\n",
        "print(f\"   • Learning Rate: {training_args.learning_rate}\")\n",
        "print(f\"   • Steps: {training_args.max_steps}\")\n",
        "print(f\"   • Eval: A cada 500 steps\")\n",
        "print(f\"   • Checkpoints: Desabilitados\")\n",
        "print(f\"   • Precisão: BFloat16\")\n",
        "print(f\"\\n⚡ Velocidade esperada: ~60 steps/min\")\n",
        "print(f\"⏱️  ETA: ~{training_args.max_steps/60:.0f} minutos\")\n",
        "print(\"=\"*100 + \"\\n\")\n",
        "\n",
        "start_time = time.time()\n",
        "\n",
        "try:\n",
        "    trainer.train()\n",
        "    print(\"\\n✅ Treinamento concluído!\\n\")\n",
        "\n",
        "except KeyboardInterrupt:\n",
        "    print(\"\\n⚠️  Treinamento interrompido pelo usuário!\\n\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"\\n❌ Erro: {e}\\n\")\n",
        "    raise\n",
        "\n",
        "finally:\n",
        "    training_time = (time.time() - start_time) / 60\n",
        "    print(f\"⏱️  Tempo de treinamento: {training_time:.1f} minutos\")\n",
        "\n",
        "    # Limpar memória\n",
        "    gc.collect()\n",
        "    torch.cuda.empty_cache()\n",
        "\n",
        "# 1️⃣3️⃣ SALVAR MODELO FINAL\n",
        "print(\"\\n\" + \"=\"*100)\n",
        "print(\"💾 SALVANDO MODELO FINAL\".center(100))\n",
        "print(\"=\"*100 + \"\\n\")\n",
        "\n",
        "final_model_path = OUTPUT_DIR / \"final_model\"\n",
        "\n",
        "try:\n",
        "    model.save_pretrained(str(final_model_path))\n",
        "    tokenizer.save_pretrained(str(final_model_path))\n",
        "\n",
        "    print(f\"✅ Modelo salvo em: {final_model_path}\\n\")\n",
        "\n",
        "    # Mostrar arquivos\n",
        "    print(\"📁 Arquivos gerados:\")\n",
        "    for file in sorted(final_model_path.iterdir()):\n",
        "        size = file.stat().st_size / (1024*1024)\n",
        "        print(f\"   • {file.name}: {size:.1f} MB\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"❌ Erro ao salvar: {e}\\n\")\n",
        "    raise\n",
        "\n",
        "# 1️⃣4️⃣ ESTATÍSTICAS FINAIS\n",
        "print(\"\\n\" + \"=\"*100)\n",
        "print(\"📊 RESUMO DO TREINAMENTO\".center(100))\n",
        "print(\"=\"*100 + \"\\n\")\n",
        "\n",
        "if hasattr(trainer.state, 'log_history') and trainer.state.log_history:\n",
        "    logs = trainer.state.log_history\n",
        "    train_losses = [log['loss'] for log in logs if 'loss' in log]\n",
        "    eval_losses = [log['eval_loss'] for log in logs if 'eval_loss' in log]\n",
        "\n",
        "    if train_losses:\n",
        "        print(\"╔\" + \"═\"*98 + \"╗\")\n",
        "        print(\"║\" + \" 📈 MÉTRICAS FINAIS \".center(98) + \"║\")\n",
        "        print(\"╠\" + \"═\"*98 + \"╣\")\n",
        "        print(f\"║  🎯 Steps: {trainer.state.global_step:,}/{training_args.max_steps:,}\".ljust(99) + \"║\")\n",
        "        print(f\"║  ⏱️  Tempo: {training_time:.1f} minutos\".ljust(99) + \"║\")\n",
        "        print(f\"║  ⚡ Velocidade: {trainer.state.global_step/training_time:.1f} steps/min\".ljust(99) + \"║\")\n",
        "        print(\"║\" + \" \"*98 + \"║\")\n",
        "        print(f\"║  📉 Loss Inicial: {train_losses[0]:.4f}\".ljust(99) + \"║\")\n",
        "        print(f\"║  📉 Loss Final: {train_losses[-1]:.4f}\".ljust(99) + \"║\")\n",
        "\n",
        "        if eval_losses:\n",
        "            print(f\"║  📊 Melhor Eval Loss: {min(eval_losses):.4f}\".ljust(99) + \"║\")\n",
        "\n",
        "        improvement = ((train_losses[0] - train_losses[-1]) / train_losses[0]) * 100\n",
        "        print(f\"║  📈 Melhoria: {improvement:.1f}%\".ljust(99) + \"║\")\n",
        "        print(\"╚\" + \"═\"*98 + \"╝\")\n",
        "\n",
        "print(\"\\n🎉 TREINAMENTO CONCLUÍDO COM SUCESSO!\")\n",
        "print(f\"💾 Modelo pronto em: {final_model_path}\")\n",
        "print(f\"📊 Datasets tokenizados em: {TOKENIZED_DIR}\")\n",
        "print(\"\\n✅ Próxima execução será mais rápida (usa cache)!\")\n",
        "print(\"=\"*100 + \"\\n\")"
      ]
    }
  ]
}